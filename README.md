# SQLEval: A diagnositic benchmark for Large Language Model

# üî• Updates

- [**2023-9-30**]: We released our [code](https://github.com/lfy79001/SQLEval)

# üîß Task
<ÊèíÂÖ•‰∏ÄÂº†‰ªªÂä°Á§∫‰æãÂõæ>

The benchmark have two main tasks: __SQL execution__ and __SQL language insturction__.


# ‚ú® Features
SQLEval is a platform for Large Language Model Evaluation (especially for LLM diagnositic). It has the following features:
- **Reasoning**: SQL contains a rich grammar in which each keyword implies a different reasoning capability, and SQLEval can effectively test model reasoning capabilities.
- **Long-Context Understanding**: The difficulty of long text evaluation is how to collect meaningful texts and tasks, our work can theoretically evaluate any long-context capability of ANY LLM context windows length. 
- **Controllable Analysis**: This platform allows fine-grained control of data generation. Such as detailed attributes of the table, fine-grained difficulty control of the generated SQL, and so on. Users have the flexibility to use it to explore more features of LLM.
- **Dynamic without data leakage**: Randomly construct synthetic data that has never been seen by LLMs, which greatly alleviates data leakage problem in LLM evaluation.


# ‚öôÔ∏è Start

##




